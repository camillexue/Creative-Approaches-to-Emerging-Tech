---
layout: post
author: Camille Xue
title: Automation, Social Algorithms, and AI
---

NOV. 13 2018

One thing that definitely stands out to me is the machine bias and how our own social discrimination can be unintentionally trained into machine learning algorithms. It seems like there is widespread use of predictive computer algorithms without validating their accuracy. I’m not even really sure how people could go about inserting proactive rules that try to go against discrimination and make changes that don’t currently exist in society, into a machine learning algorithm. I want to know more about what sorts of wrong programs and algorithms are being used right now and have more credibility than they should. 

I’m also really interested in the idea of humans trusting computers more than they should, and becoming more complacent as a result of relying on automated things more and more. What are some ways that students and Olin have lost some skill as a result of trusting a computer to do something? 

The social credit score in China was also particularly interesting because it obviously seems like a scary, bad thing to not do…  but also I think if it was done correctly it would be a good thing? I think I’m only afraid of things like automated job candidate screening and social credit scores because I think they will somehow overlook something that a real person wouldn’t, that it would be more limiting than a human would be. But if it wasn’t, and it proved to actually be more fair, and look at people more wholistically, wouldn’t that be a good thing?
